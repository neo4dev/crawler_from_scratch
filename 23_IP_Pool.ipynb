{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp IpPool\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代理池\n",
    "实现效果\n",
    "1. 自动抓取新ip\n",
    "* 自动删除无效ip（根据健康度）\n",
    "\n",
    "任务：\n",
    "* 显示优质ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import re,random,time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import requests,redis \n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取一个可用ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def connect_db() -> object:\n",
    "    connection_pool = redis.ConnectionPool(host='localhost', port=6379, decode_responses=True)\n",
    "    rdb = redis.Redis(connection_pool=connection_pool)\n",
    "    return rdb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "rdb = connect_db()\n",
    "def _get_ip(protocal='http') -> str:\n",
    "    '把health作为权重，随机抽取ip'\n",
    "    prim_ips = rdb.zrange(protocal,-20,-1)    \n",
    "    return random.choice(prim_ips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_data():\n",
    "    rdb = connect_db()\n",
    "    rdb.zadd('http',{'39.137.107.9:8080':10}) \n",
    "    rdb.zadd('https',{'39.137.107.9:8080':10})        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'39.137.69.10:8080'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make_test_data()\n",
    "_get_ip()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('91.205.174.26:80', 100.0),\n",
       " ('39.137.69.10:80', 103.0),\n",
       " ('58.61.154.153:8080', 115.0),\n",
       " ('115.223.7.110:80', 130.0),\n",
       " ('116.114.19.204:443', 248.0),\n",
       " ('59.56.28.254:80', 303.0),\n",
       " ('39.137.107.98:80', 365.0),\n",
       " ('39.137.69.7:80', 380.0),\n",
       " ('61.147.210.159:82', 515.0),\n",
       " ('39.137.69.10:8080', 798.0)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prim_ips = rdb.zrange('http',-10,-1,withscores=True)\n",
    "prim_ips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = proxy_get('http://www.woshipm.com/category/zhichang/page/4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新健康值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def update_health(ip,is_health=False,protocal='http') -> float:\n",
    "    increase = 1 if is_health else -int(rdb.zscore(protocal,ip)/2)\n",
    "    result = rdb.zincrby(protocal,increase,ip)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_health('39.137.107.9:8080',is_health=False,protocal='https')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬取ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "proxy_website_urls = '''\n",
    "https://www.kuaidaili.com/free/inha/\n",
    "http://www.nimadaili.com/gaoni/\n",
    "http://www.xiladaili.com/gaoni/\n",
    "https://ip.jiangxianli.com/?anonymity=2\n",
    "https://www.7yip.cn/free/\n",
    "http://www.ip3366.net/free/\n",
    "https://list.proxylistplus.com/Fresh-HTTP-Proxy-List-1\n",
    "http://proxyslist.com/\n",
    "'''.strip().split('\\n')\n",
    "\n",
    "def match_ip(tag): return re.match(r'^(\\d{1,3}\\.){3}\\d{1,3}$',tag.text.strip())\n",
    "def match_port(tag): return re.match(r'^\\d{2,5}$',tag.text.strip())\n",
    "def match_ip_with_port(tag): return re.match(r'^(\\d{1,3}\\.){3}\\d{1,3}:\\d{2,5}$',tag.text.strip())\n",
    "\n",
    "def find_port(ip_item_soup) -> str:\n",
    "    soup = ip_item_soup\n",
    "    while True:\n",
    "        # 不停的查找包含port的父级\n",
    "        soup = soup.parent\n",
    "        if len(soup.find_all(match_ip)) > 1:\n",
    "#             print('解析port失败',soup)\n",
    "            return \n",
    "        if soup.find(match_port): \n",
    "            return soup.find(match_port).text.strip()\n",
    "\n",
    "def find_ips(soup) -> iter:\n",
    "    '从soup中解析出ip和port'\n",
    "    # 39.137.107.98:80这种情况\n",
    "    if soup.find_all(match_ip_with_port):\n",
    "        for item in soup.find_all(match_ip_with_port):\n",
    "            yield item.text.strip()\n",
    "    # 39.137.107.98 | 80这种情况\n",
    "    elif soup.find_all(match_ip):\n",
    "        for item in soup.find_all(match_ip):\n",
    "            ip = item.text.strip()\n",
    "            port = find_port(item)\n",
    "            if port: yield ip+':'+port\n",
    "    else:\n",
    "        print('解析失败：',soup)\n",
    "        \n",
    "        \n",
    "# 这里没想好，到底http和https都爬，然后自动切换还是手动\n",
    "def crawl_ip(url,protocal='http'):\n",
    "    '爬取1个页面的ip'\n",
    "    increase = 0\n",
    "    \n",
    "    res = requests.get(url,headers={'user-agent':'Mozilla/5.0'})\n",
    "    if res.status_code == 200:\n",
    "        soup = BeautifulSoup(res.text,features='lxml')\n",
    "        for ip in find_ips(soup):\n",
    "            \n",
    "            if rdb.zadd('http',{ip:100},nx=True):\n",
    "                increase += 1\n",
    "        stock = rdb.zcount(protocal,0,100000)\n",
    "        print(f'{url} 新增：{increase}，库存更新为：{stock}个')\n",
    "    else:\n",
    "        print(url,res,'requests请求失败')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.nimadaili.com/gaoni/ 新增：0，库存更新为：2687个\n"
     ]
    }
   ],
   "source": [
    "crawl_ip(proxy_website_urls[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 校验IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def validate(ip,url='http://m.sm.cn/',timeout=5) -> float:\n",
    "    protocal = url.split(':')[0]\n",
    "    proxies={protocal: protocal+'://'+ip}\n",
    "    try:\n",
    "        res = requests.get(url,\n",
    "                           headers={'user-agent':'Mozilla/5.0'},\n",
    "                           proxies=proxies,\n",
    "                           timeout=timeout)\n",
    "    except:\n",
    "        return update_health(ip,is_health=False,protocal=protocal)\n",
    "    else:\n",
    "        if res and res.status_code == 200:\n",
    "            return update_health(ip,is_health=True,protocal=protocal)\n",
    "        else:\n",
    "            return update_health(ip,is_health=False,protocal=protocal)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate('128.199.246.10:44344')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定期更新IP\n",
    ">5min更新一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "last_crawl = 0\n",
    "def parallel_crawl_ips():\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        executor.map(crawl_ip, proxy_website_urls) \n",
    "def repeat_crawl_ips(frequency=300):\n",
    "    global last_crawl\n",
    "    now = time.time()\n",
    "    if last_crawl//frequency != now//frequency:\n",
    "        last_crawl = now\n",
    "        parallel_crawl_ips()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.7yip.cn/free/ 新增：0，库存更新为：2708个\n",
      "http://www.xiladaili.com/gaoni/ 新增：17，库存更新为：2708个\n",
      "http://www.ip3366.net/free/ 新增：1，库存更新为：2708个\n",
      "http://www.nimadaili.com/gaoni/ 新增：3，库存更新为：2708个\n",
      "https://www.kuaidaili.com/free/inha/ 新增：0，库存更新为：2708个\n",
      "https://ip.jiangxianli.com/?anonymity=2 新增：1，库存更新为：2709个\n",
      "http://proxyslist.com/ 新增：17，库存更新为：2726个\n",
      "https://list.proxylistplus.com/Fresh-HTTP-Proxy-List-1 新增：0，库存更新为：2726个\n"
     ]
    }
   ],
   "source": [
    "repeat_crawl_ips()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定期删除IP\n",
    "> 每日删除health小于50的IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# 这里也是，没想好，怎么维护http和https两个库\n",
    "\n",
    "last_delete = 0\n",
    "def delete_ips(protocal='http'):\n",
    "    result = rdb.zremrangebyscore(protocal,0,50) \n",
    "    return result\n",
    "    \n",
    "def repeat_delete_ips(frequency=24*3600):\n",
    "    global last_delete\n",
    "    now = time.time()\n",
    "    \n",
    "    if last_delete//frequency != now//frequency:\n",
    "        last_delete = now\n",
    "        result = delete_ips()\n",
    "        print('移除：',result,'个IP')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_delete_ips()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动维护IP池"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ip(protocal='http') -> str:\n",
    "    repeat_crawl_ips()\n",
    "    repeat_delete_ips()\n",
    "    return _get_ip(protocal)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.7yip.cn/free/ 新增：10，库存更新为：369个\n",
      "http://www.ip3366.net/free/ 新增：13，库存更新为：382个\n",
      "http://www.nimadaili.com/gaoni/ 新增：42，库存更新为：426个\n",
      "https://www.kuaidaili.com/free/inha/ 新增：13，库存更新为：439个\n",
      "http://www.xiladaili.com/gaoni/ 新增：2，库存更新为：439个\n",
      "https://ip.jiangxianli.com/?anonymity=2 新增：8，库存更新为：447个\n",
      "http://proxyslist.com/ 新增：20，库存更新为：467个\n",
      "https://list.proxylistplus.com/Fresh-HTTP-Proxy-List-1 新增：48，库存更新为：515个\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'115.223.7.110:80'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代理请求\n",
    "1. 失败换ip重新请求\n",
    "* 超过10次log下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def proxy_get(url) -> object:\n",
    "    repeat_count = 0\n",
    "    while repeat_count<30:\n",
    "        repeat_count += 1\n",
    "\n",
    "        protocal = url.split(':')[0]\n",
    "        ip = get_ip(protocal=protocal)\n",
    "        proxies={protocal: protocal+'://'+ip}\n",
    "        \n",
    "        try:\n",
    "            res = requests.get(url,\n",
    "                               headers={'user-agent':'Mozilla/5.0'},\n",
    "                               proxies=proxies,\n",
    "                               allow_redirects=False,\n",
    "                               timeout=5)\n",
    "        except:\n",
    "            # 报错 重来 health-\n",
    "            update_health(ip,False,protocal)            \n",
    "        else:\n",
    "            # 有自动跳转的，合适跳转链接后再执行\n",
    "            if res and res.status_code == 301:\n",
    "                if 'Location' in res.headers and res.headers['Location'] in url:\n",
    "                    return proxy_get(res.headers['Location'])\n",
    "                else:\n",
    "                    print('\\n\\n!301 error',url,res,res.headers)\n",
    "                    return\n",
    "            # 有些200的text中确实bad request\n",
    "            if res and res.status_code == 200 and len(res.text)>100:\n",
    "                # 200 & html存在，则保存 break health+\n",
    "                update_health(ip,True,protocal)            \n",
    "#                 print('success:',url,'try times:',repeat_count)\n",
    "                if repeat_count > 4: print('try too much times:',url,repeat_count,'healthy ips:',rdb.zcount(protocal,101,10000))\n",
    "                return res\n",
    "            elif res and res.status_code == 404:\n",
    "                # 404 log break health+\n",
    "                with open('rrpm.log','a') as f:\n",
    "                    f.write(f'{url} 404\\n')\n",
    "                update_health(ip,True,protocal)  \n",
    "                break \n",
    "            else:\n",
    "                # 其他,比如访问过快 重来 health-\n",
    "                update_health(ip,False,protocal)   \n",
    "    print('\\n\\n!overtry:',url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.nimadaili.com/gaoni/ 新增：41，库存更新为：2892个\n",
      "https://www.7yip.cn/free/ 新增：0，库存更新为：2892个\n",
      "http://www.ip3366.net/free/ 新增：0，库存更新为：2892个\n",
      "http://www.xiladaili.com/gaoni/ 新增：2，库存更新为：2894个\n",
      "https://ip.jiangxianli.com/?anonymity=2 新增：1，库存更新为：2895个\n",
      "https://www.kuaidaili.com/free/inha/ 新增：0，库存更新为：2895个\n",
      "http://proxyslist.com/ 新增：17，库存更新为：2912个\n",
      "https://list.proxylistplus.com/Fresh-HTTP-Proxy-List-1 新增：0，库存更新为：2912个\n"
     ]
    }
   ],
   "source": [
    "res = proxy_get('http://www.woshipm.com/category/zhichang/page/1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 并发任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parallel_task(fn,task_arg_list,max_workers=100,show_progress=True) -> list:\n",
    "    data_list = []\n",
    "    completed_num = 0\n",
    "    show_progress_at = 0\n",
    "    \n",
    "    task_num = len(task_arg_list)\n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        for data in executor.map(fn,task_arg_list):\n",
    "            data_list.append(data)\n",
    "            \n",
    "            if show_progress:\n",
    "                completed_num += 1\n",
    "                now = time.time()\n",
    "                if now//5 != show_progress_at//5:\n",
    "                    show_progress_at = now\n",
    "                    print('progress:{:.2%}'.format(completed_num/task_num))     \n",
    "    end_time = time.time()\n",
    "    if show_progress: print('time cost:',int(end_time - start_time),'s',len(task_arg_list),'urls')\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 发布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 23_IP_Pool.ipynb.\r\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "!nbdev_build_lib --fname 23_IP_Pool.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master d91c22a] improve delete ip\r\n",
      " 1 file changed, 22 insertions(+), 16 deletions(-)\r\n"
     ]
    }
   ],
   "source": [
    "!git add 23_IP_Pool.ipynb\n",
    "!git commit -m \"improve delete ip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redis.exceptions.ConnectionError: Error 8 connecting to localhost:6379. nodename nor servname provided, or not known.\n",
    "# 由于redis给没有用户名的用户，有连接数限制\n",
    "!ulimit -n 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
