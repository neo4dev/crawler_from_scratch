# AUTOGENERATED! DO NOT EDIT! File to edit: 23_IP_Pool.ipynb (unless otherwise specified).

__all__ = ['connect_db', 'update_health', 'match_ip', 'match_port', 'match_ip_with_port', 'find_port', 'find_ips',
           'crawl_ip', 'proxy_website_urls', 'https', 'validate', 'parallel_validate', 'parallel_crawl_ips',
           'repeat_crawl_ips', 'last_crawl', 'delete_ips', 'repeat_delete_ips', 'last_delete', 'get_ip']

# Cell
import json,re,random,sys,time,os
from concurrent.futures import ThreadPoolExecutor

import requests,redis
from bs4 import BeautifulSoup,Tag,NavigableString
from collections import Counter


# Cell
def connect_db() -> object:
    connection_pool = redis.ConnectionPool(host='localhost', port=6379, decode_responses=True)
    rdb = redis.Redis(connection_pool=connection_pool)
    return rdb

def _get_ip(protocal='http') -> str:
    '把health作为权重，随机抽取ip'
    rdb = connect_db()
    ips = []
    healths = []

    for name in rdb.keys():
        health_type = 'http_health' if protocal=='http' else 'https_health'
        health = rdb.hget(name,health_type)

        ips.append(name)
        healths.append(int(health))
    return random.choices(ips,healths)[0]

# Cell
def update_health(ip,is_health=False,protocal='http'):
    rdb = connect_db()
    health_type = 'http_health' if protocal=='http' else 'https_health'

    health = int(rdb.hget(ip,health_type))
    if is_health:
        rdb.hset(ip,health_type,health+1)
        print('+ Health',protocal,ip,rdb.hget(ip,health_type))
    else:
        rdb.hset(ip,health_type,health//2)
#     print('+' if is_health else '-','Health',protocal,ip,rdb.hget(ip,health_type))

# Cell
proxy_website_urls = '''
https://www.kuaidaili.com/free/inha/
http://www.nimadaili.com/gaoni/
https://www.xicidaili.com/nn/
http://www.xiladaili.com/gaoni/
https://ip.jiangxianli.com/?anonymity=2
https://www.7yip.cn/free/
http://www.ip3366.net/free/
https://list.proxylistplus.com/Fresh-HTTP-Proxy-List-1
http://proxyslist.com/
'''.strip().split('\n')

def match_ip(tag): return re.match(r'^(\d{1,3}\.){3}\d{1,3}$',tag.text.strip())
def match_port(tag): return re.match(r'^\d{2,5}$',tag.text.strip())
def match_ip_with_port(tag): return re.match(r'^(\d{1,3}\.){3}\d{1,3}:\d{2,5}$',tag.text.strip())

def find_port(ip_item_soup) -> str:
    soup = ip_item_soup
    while True:
        # 不停的查找包含port的父级
        soup = soup.parent
        if len(soup.find_all(match_ip)) > 1:
#             print('解析port失败',soup)
            return
        if soup.find(match_port):
            return soup.find(match_port).text.strip()

def find_ips(soup) -> iter:
    '从soup中解析出ip和port'
    # 39.137.107.98:80这种情况
    if soup.find_all(match_ip_with_port):
        for item in soup.find_all(match_ip_with_port):
            yield item.text.strip()
    # 39.137.107.98 | 80这种情况
    elif soup.find_all(match_ip):
        for item in soup.find_all(match_ip):
            ip = item.text.strip()
            port = find_port(item)
            if port: yield ip+':'+port
    else:
        print('解析失败：',soup)

def crawl_ip(url):
    '爬取1个页面的ip'
    rdb = connect_db()
    res = requests.get(url,headers={'user-agent':'Mozilla/5.0'})
    stock_before = len(rdb.keys())

    if res.status_code == 200:
        soup = BeautifulSoup(res.text)
        for ip in find_ips(soup):
            rdb.hmset(ip,{'http_health':100,'https_health':100})

        stock = len(rdb.keys())
        print(f'{url} 新增：{stock-stock_before}，库存更新为：{stock}个')
    else:
        print(url,res,'requests请求失败')

# Cell
def validate(ip,url='http://m.sm.cn/',timeout=5):
    protocal = url.split(':')[0]
    proxies={protocal: protocal+'://'+ip}
    try:
        res = requests.get(url,
                           headers={'user-agent':'Mozilla/5.0'},
                           proxies=proxies,
                           timeout=timeout)
    except:
        update_health(ip,is_health=False,protocal=protocal)
    else:
        if res and res.status_code == 200:
            update_health(ip,is_health=True,protocal=protocal)
        else:
            update_health(ip,is_health=False,protocal=protocal)


# Cell
def parallel_validate(max_workers=100):
    rdb = connect_db()
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        executor.map(validate, rdb.keys())

# Cell
last_crawl = 0
def parallel_crawl_ips():
    with ThreadPoolExecutor(max_workers=10) as executor:
        executor.map(crawl_ip, proxy_website_urls)
def repeat_crawl_ips(frequency=600):
    global last_crawl
    now = time.time()
    if last_crawl//frequency != now//frequency:
        last_crawl = now
        parallel_crawl_ips()

# Cell
last_delete = 0
def delete_ips():
    rdb = connect_db()
    count = 0
    for name in rdb.keys():
        if rdb.hget(name,'http_health') == '0' and rdb.hget(name,'https_health') == '0':
            rdb.delete(ip)
            count += 1
    print(f'删除{count}个无效IP')

def repeat_delete_ips(frequency=24*3600):
    global last_delete
    now = time.time()

    if last_delete//frequency != now//frequency:
        last_delete = now
        delete_ips()


# Cell
def get_ip(protocal='http') -> str:
    repeat_crawl_ips()
    repeat_delete_ips()
    return _get_ip(protocal)
