# AUTOGENERATED! DO NOT EDIT! File to edit: 11_Proxy_Request.ipynb (unless otherwise specified).

__all__ = ['update_ip_pool', 'load_ip_pool', 'proxy_get', 'init_ip_dict', 'update_ip_health', 'smart_proxy_get',
           'validate_ip', 'validate_ip_dict', 'get_healthy_ip_pool']

# Cell
import requests,json,re,random,sys,time
from bs4 import BeautifulSoup,Tag,NavigableString

from .utils import *

from concurrent.futures import ThreadPoolExecutor
import pandas as pd


# Cell
def update_ip_pool(protocol='https'):
    '爬取并保存ip'
    soup_list = []
    for i in range(1,11):
        url = f'https://www.freeip.top/?page={i}&protocol={protocol}'
        print('正在爬取:',url)
        headers={'user-agent':'Mozilla/5.0'}

        try_times = 1
        while try_times < 6:
            res = requests.get(url,headers=headers)
            if res.status_code == 200:
                soup = BeautifulSoup(res.text,'lxml')
                soup_list.append(soup)
                break
            else:
                print(f'第{try_times}次尝试失败：',url,res,res.text)
                try_times += 1
        time.sleep(1)
    ip_pool = []
    for s in soup_list:
        main_content = s.find('tbody')
        items = main_content.find_all(class_='btn-copy')
        ips = [item['data-url'] for item in items]
        ip_pool += ips

    ip_pool = list(set(ip_pool))
    with open('./data/11_Proxy.json', 'w') as f:
        json.dump(ip_pool,f)

    print(len(ip_pool),'条IP更新成功！')
    return ip_pool

# Cell
def load_ip_pool(path='./data/11_Proxy.json'):
    '加载之前备份的ip'
    with open(path, 'r') as f:
        ip_pool = json.loads(f.read())
    print('IP数:',len(ip_pool))
    return ip_pool

# Cell
def proxy_get(url,ip,show_log=True):
    '根据ip代理访问'
    headers={'user-agent':'Mozilla/5.0'}

    protocol = 'https' if 'https' in ip else 'http'
    proxies = {protocol: ip}

    if show_log:
        print(ip,'代理中……')
    try:
        res = requests.get(url,proxies=proxies,headers=headers,timeout=5)
        return res
    except:
#         print(f'error: {sys.exc_info()}\n')
        return requests.Response()


# Cell
def init_ip_dict(ip_pool):
    '初始化所有ip的健康值'
    ip_dict = {}
    for ip in ip_pool:
        ip_dict[ip] = 50
    return ip_dict

# Cell
def update_ip_health(res,ip,ip_dict):
    if res.status_code == 200:
        ip_dict[ip] += 1
        print(ip,'健康值更新为：',ip_dict[ip])
        return True
    else:
        ip_dict[ip] = int(ip_dict[ip]/2)
        print(ip,'健康值更新为：',ip_dict[ip])
        return False

def smart_proxy_get(url,ip_dict):
    '如果ip无效，可以自动更换，并记录ip健康值；如果更换10次都无效，则终止'
    try_times = 1
    while try_times < 11:
        # 选择健康值最好的10个ip
        ips = sorted(ip_dict,key=lambda k:ip_dict[k],reverse=True)[:10]
#         print(f'第{try_times}次尝试')
        ip = random.choice(ips)
        res = proxy_get(url,ip,show_log=False)
        if update_ip_health(res,ip,ip_dict):
            print('成功:',url)
            return res
        else:
#             print(res,res.text)
            try_times += 1

    print('失败:',url)
    return res

# Cell
def validate_ip(ip,ip_dict,url='https://www.baidu.com/'):
    res = proxy_get(url,ip,show_log=False)
    update_ip_health(res,ip,ip_dict)

def validate_ip_dict(ip_dict,times=3,url='https://www.baidu.com/'):
    for i in range(times):
        with ThreadPoolExecutor(max_workers=50) as executor:
            executor.map(lambda ip : validate_ip(ip,ip_dict,url), ip_dict)
    return ip_dict

# Cell
def get_healthy_ip_pool(protocol='https',update_now=False):
    if update_now:
        update_ip_pool(protocol)
    ip_list = load_ip_pool()
    ip_dict = init_ip_dict(ip_list)
    validate_ip_dict(ip_dict)
    return ip_dict