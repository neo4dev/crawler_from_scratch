# AUTOGENERATED! DO NOT EDIT! File to edit: 11_Proxy_Request.ipynb (unless otherwise specified).

__all__ = ['update_ip_pool', 'load_ip_pool', 'proxy_get', 'init_ip_dict', 'update_ip_health', 'smart_proxy_get',
           'validate_ip', 'validate_ip_dict']

# Cell
%reload_ext autoreload
%autoreload 2
import requests,json,re,random,sys
from bs4 import BeautifulSoup,Tag,NavigableString

from .utils import *

from concurrent.futures import ThreadPoolExecutor
import pandas as pd


# Cell
def update_ip_pool():
    '爬取并保存ip'
    soup_list = []
    for i in range(1,11):
        url = f'https://www.freeip.top/?page={i}&protocol=https'
        print('正在爬取:',url)
        headers={'user-agent':'Mozilla/5.0'}
        res = requests.get(url,headers=headers)
        soup = BeautifulSoup(res.text)
        soup_list.append(soup)

    ip_pool = []
    for s in soup_list:
        main_content = s.find('tbody')
        items = main_content.find_all(class_='btn-copy')
        ips = [item['data-url'] for item in items]
        ip_pool += ips

    ip_pool = list(set(ip_pool))
    with open('./data/11_Proxy.json', 'w') as f:
        json.dump(ip_pool,f)

    print(len(ip_pool),'条IP更新成功！')
    return ip_pool

# Cell
def load_ip_pool():
    '加载之前备份的ip'
    with open('./data/11_Proxy.json', 'r') as f:
        ip_pool = json.loads(f.read())
    print('IP数:',len(ip_pool))
    return ip_pool

# Cell
def proxy_get(url,ip):
    '根据ip代理访问'
    headers={'user-agent':'Mozilla/5.0'}

    proxies = {'https': ip}
    print(ip,'代理中……')
    try:
        res = requests.get(url,proxies=proxies,headers=headers,timeout=5)
        return res
    except:
#         print(f'error: {sys.exc_info()}\n')
        return requests.Response()


# Cell
def init_ip_dict(ip_pool):
    '初始化所有ip的健康值'
    ip_dict = {}
    for ip in ip_pool:
        ip_dict[ip] = 50
    return ip_dict

# Cell
def update_ip_health(res,ip,ip_dict):
    if res.status_code == 200:
        ip_dict[ip] += 1
        print(ip,'健康值更新为：',ip_dict[ip])
        return True
    else:
        ip_dict[ip] = int(ip_dict[ip]/2)
        print(ip,'健康值更新为：',ip_dict[ip])
        return False

def smart_proxy_get(url,ip_dict):
    '如果ip无效，可以自动更换，并记录ip健康值；如果更换10次都无效，则终止'
    try_times = 1
    while try_times < 11:
        # 选择健康值最好的10个ip
        ips = sorted(ip_dict,key=lambda k:ip_dict[k],reverse=True)[:10]
        print(f'第{try_times}次尝试')
        ip = random.choice(ips)
        res = proxy_get(url,ip)
        if update_ip_health(res,ip,ip_dict):
            print('成功！')
            return res
        else:
#             print(res,res.text)
            try_times += 1

    print('10次尝试扔失败:',url)
    return res

# Cell
def validate_ip(ip,ip_dict,url='https://www.baidu.com/'):
    res = proxy_get(url,ip)
    update_ip_health(res,ip,ip_dict)

def validate_ip_dict(ip_dict,times=3,url='https://www.baidu.com/'):
    for i in range(times):
        with ThreadPoolExecutor(max_workers=50) as executor:
            executor.map(lambda ip : validate_ip(ip,ip_dict,url), ip_dict)
    return ip_dict